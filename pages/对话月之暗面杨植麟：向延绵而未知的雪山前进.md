title:: 对话月之暗面杨植麟：向延绵而未知的雪山前进
author:: [[张小珺]]
url:: https://mp.weixin.qq.com/s/qVXcyw96IEPjrvZeA_1VMQ
tags:: #[[ai]] #[[Readwise]]  
![](https://mmbiz.qpic.cn/mmbiz_jpg/y4LZ1iawaf3mnmTUw35w1FB9zKde0kh4glFcxFPYN0bzYq1J9LUx2nm0ibhmIsFz2khp6N3ibN6jRw0fTTltOzuMQ/0?wx_fmt=jpeg)

- > 应该看什么是大方向、大梯度。当你眼前有十条路，一般人考虑我走这条路前面有一个行人怎么刹车，是短期细节，但这十条路到底选哪一条最重要。
  
  这个领域在之前有这样的问题。比如，在只有一两百万token（标记）的数据集上，看perplexity（困惑度，衡量模型在预测序列时的不确定性或混乱度）怎么降得更低，loss（损失，模型在训练过程中的误差或损失函数的值）怎么降得更低，怎么提升准确率，你会陷入无限雕花。有人发明很多诡异的architecture（架构），这些是雕花技巧。雕花之后可能在这种数据集上变好，但没看到问题本质。
  
  本质在于，要去分析这个领域缺少的是什么？第一性原理是什么？
  
  Scaling law为什么能成为第一性原理？你只要能找到一个结构，满足两个条件：一是足够通用，二是可规模化。通用是你把所有问题放到这个框架建模，可规模化是只要你投入足够多算力，它就能变好。
  
  这是我在Google学到的思维：如果能被更底层的东西解释，就不应该在上层过度雕花。有一句重要的话我很认同：如果你能用scale解决的问题，就不要用新的算法解决。新算法最大价值是让它怎么更好的scale。当你把自己从雕花的事中释放出来，可以看到更多。 ([View Highlight](https://read.readwise.io/read/01hr508c7h7mbek25mfrwxekpt))
- > 老的计算机内存，在过去几十年涨了好几个数量级，一样的事会发生在新的计算机上。它能解决很多现在的问题。比如，现在多模态架构还需要tokenizer（标记器），但当你有一个无损压缩的long context就不需要了，可以把原始的放进去。进一步讲，它是把新计算范式变成更通用的基础。 ([View Highlight](https://read.readwise.io/read/01hr50de6m2k48kpcgpbb6472k))
- > 第二，能够做到个性化。AI最核心的价值是个性化互动，价值落脚点还是个性化，AGI会比上一代推荐引擎更加个性化。
  
  但个性化过程不是通过微调实现，而是它能支持很长的context（上下文）。你跟机器所有的历史都是context，这个context定义了个性化过程，而且无法被复刻，它会是更直接的对话，对话产生信息。 ([View Highlight](https://read.readwise.io/read/01hr50e107bmnq6yhmwhcagb1w))
- > 另一方面是，你不能只提升窗口，不能只看数字，今天是几百万还是多少亿的窗口没有意义。你要看它在这个窗口下能实现的推理能力、the faithfulness的能力（对原始信息的忠实度）、the instruction following的能力（遵循指令的能力）——不应该只追求单一指标，而是结合指标和能力。 ([View Highlight](https://read.readwise.io/read/01hr50enktvs0xyts7x52dq3e8))
- > 如果这两个维度持续提升，能做非常多事。可能可以follow（执行）一个几万字的instruction（指令），instruction本身会定义很多agent（智能体），高度个性化。 ([View Highlight](https://read.readwise.io/read/01hr50f17akyzxq4hsf6may2gd))
- > 接下来会有两个大的milestone（里程碑）。一是真正的统一的世界模型，就是它能统一各种不同模态，一个真正的scalable和general的architecture（可扩展、通用的系统结构）。
  
  二是能在没有人类数据输入的情况下，使AI持续进化。 ([View Highlight](https://read.readwise.io/read/01hr50hc4f4rfszmx79ch2zksj))
- > 我们会去做这两件事。剩下很多问题，都是这两个因素推导出来的。今天谈到reasoning（推理）、agent（智能体），都是这两个问题解决后的产物。要再做一些雕花，但没有fundamental的blocker（根本性阻碍因素）。 ([View Highlight](https://read.readwise.io/read/01hr50jq4kmac6m45z1gyk8g7x))
- > 主要瓶颈，核心还是数据，你怎么去规模化地拟合这个数据？之前没被验证过。特别是，当你的动作比较复杂，生成的效果photo realistic（照片逼真）。在这样的条件下，能够去规模化，它这次解决了这些。
  
  剩下的是它也没有完全解决，比如需要一个统一的architecture（架构）。DiT这个architecture仍然不是非常通用。在单纯对视觉信号的marginal probability（边际概率）去建模，它可以做得非常好，但怎么泛化成一个通用的新计算机？还是需要更unified architecture（统一的架构），这个东西还是有空间。 ([View Highlight](https://read.readwise.io/read/01hr50skad09j45cn545f7ambz))
- > 没有本质瓶颈。当token space足够大，变成一个新型计算机解决通用性问题就OK了，它就是一个通用世界模型。
  
  （他这么说）很重要一点在于，大家都能看到现在的局限性。但解决方式并不一定需要全新框架。AI唯一work就是next token prediction + scaling law，只要token足够完整，都是可以做的。当然今天他指出的问题存在，但这些问题就是你把token space变得很通用，就可以了。 ([View Highlight](https://read.readwise.io/read/01hr5111nnhepe5rz6smqqvzxg))
- > **腾讯新闻：幻觉的问题怎么解决？**
  
  **杨植麟：**还是scaling law，就是scale的是不一样的东西。 ([View Highlight](https://read.readwise.io/read/01hr511j5txswj4mnvk77nkkf9))
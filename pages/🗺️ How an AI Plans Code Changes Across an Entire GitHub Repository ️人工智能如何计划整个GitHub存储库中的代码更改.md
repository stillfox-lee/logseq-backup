title:: ğŸ—ºï¸ How an AI Plans Code Changes Across an Entire GitHub Repository ï¸äººå·¥æ™ºèƒ½å¦‚ä½•è®¡åˆ’æ•´ä¸ªGitHubå­˜å‚¨åº“ä¸­çš„ä»£ç æ›´æ”¹
author:: [[sweep.dev]]
url:: https://docs.sweep.dev/blogs/ai-code-planning

![](https://docs.sweep.dev/og_image.png)

- > We've tried a lot of strategies to manipulate and improve the input context. We encountered an interesting issue everytime we added additional inputs. This will hurt our performance in the short term because of how GPT4 currently degrades in performance (both planning, coding, and instruction following) after ~20k tokens.  
  æˆ‘ä»¬å·²ç»å°è¯•äº†å¾ˆå¤šç­–ç•¥æ¥æ“ä½œå’Œæ”¹è¿›è¾“å…¥ä¸Šä¸‹æ–‡ã€‚æ¯æ¬¡æ·»åŠ é¢å¤–çš„è¾“å…¥æ—¶ï¼Œæˆ‘ä»¬éƒ½ä¼šé‡åˆ°ä¸€ä¸ªæœ‰è¶£çš„é—®é¢˜ã€‚è¿™å°†åœ¨çŸ­æœŸå†…æŸå®³æˆ‘ä»¬çš„æ€§èƒ½ï¼Œå› ä¸ºGPT4ç›®å‰åœ¨å¤§çº¦20kä»¤ç‰Œä¹‹åçš„æ€§èƒ½ï¼ˆè§„åˆ’ï¼Œç¼–ç å’ŒæŒ‡ä»¤éµå¾ªï¼‰ä¼šä¸‹é™ã€‚
  
  This is a prevalent effect known as lost-in-the-middle (LIM) where LLMs only focus on the context at the beginning and end of the prompt. Thus, when we try to add more context, GPT4 will also ignore a lot of it. ([View Highlight](https://read.readwise.io/read/01j2xbxm2ayfxw5q4j7zcgpvdg))
- > We used the CST of python to extract all spans that define or mention a certain class, function, or class method AKA entity (we can just check if `entity in span`). This is not as accurate as static analysis tools like LSPs and type-checkers, but it's a 99% solution. After getting this we can prune each file to only whatâ€™s needed to make the code change to it. But how do we get the entities?
  
  We can use the above graph with the initial searched files as the root nodes.
  
  Now we have an initial node set, and we can expand this out by one degree. We clearly canâ€™t fit all of itâ€™s degree 2 neighbors in context, but the extracted entities can fit in context. Ideally the ones that lie within the extracted snippets will be mentioned, and we can prune a set that is guaranteed to have perfect recall.
  
  `Set with high recall and low precision -> an intelligent LLM workflow -> Set with high recall and high precision`
  
  The LLM is really good at finding the items that are truly relevant, and we need to have high recall and precision. Now we have all of this relevant context, and instructions to be passed to the execution (diff generation and validation). ([View Highlight](https://read.readwise.io/read/01j2xfaemb5gk28qmcd7z6mqcd))
	- é€šè¿‡ AST å›¾ç©ºé—´ç»“åˆ entity æ£€ç´¢æ¥æ‰¾åˆ°æ›´åŠ å…³é”®çš„ contextã€‚
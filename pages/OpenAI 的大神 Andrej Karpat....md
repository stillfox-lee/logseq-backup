title:: OpenAI 的大神 Andrej Karpat...
author:: [[@dotey on Twitter]]
url:: https://twitter.com/dotey/status/1728959646138880026

![](https://pbs.twimg.com/profile_images/561086911561736192/6_g58vEs.jpeg)

- > OpenAI 的大神 Andrej Karpathy 前几天在他的 YouTube 频道讲了一堂课，系统的介绍了大语言模型，内容深入浅出，非常赞，抽空将它翻译成了双语，由于内容较长，我将分批上传，以下是第一部分精校后的双语视频，字幕文稿如下：
  
  Intro: Large Language Model (LLM) talk
  
  大家好。最近，我进行了一场关于大语言模型的 30 分钟入门讲座。遗憾的是，这次讲座没有被录制下来，但许多人在讲座后找到我，他们告诉我非常喜欢那次讲座。因此，我决定重新录制并上传到 YouTube，那么，让我们开始吧，为大家带来“忙碌人士的大语言模型入门”系列，主讲人 Scott。好的，那我们开始吧。
  
  LLM Inference
  
  首先，什么是大语言模型 (Large Language Model) 呢？其实，一个大语言模型就是由两个文件组成的。在这个假设的目录中会有两个文件。
  
  以 Llama 2 70B 模型为例，这是一个由 Meta AI 发布的大语言模型。这是 Llama 系列语言模型的第二代，也是该系列中参数最多的模型，达到了 700 亿。LAMA2 系列包括了多个不同规模的模型，70 亿，130 亿，340 亿，700 亿是最大的一个。
  
  现在很多人喜欢这个模型，因为它可能是目前公开权重最强大的模型。Meta 发布了这款模型的权重、架构和相关论文，所以任何人都可以很轻松地使用这个模型。这与其他一些你可能熟悉的语言模型不同，例如，如果你正在使用 ChatGPT 或类似的东西，其架构并未公开，是 OpenAI 的产权，你只能通过网页界面使用，但你实际上没有访问那个模型的权限。
  
  在这种情况下，Llama 2 70B 模型实际上就是你电脑上的两个文件：一个是存储参数的文件，另一个是运行这些参数的代码。这些参数是神经网络（即语言模型）的权重或参数。我们稍后会详细解释。因为这是一个拥有 700 亿参数的模型，每个参数占用两个字节，因此参数文件的大小为 140 GB，之所以是两个字节，是因为这是 float 16 类型的数据。
  
  除了这些参数，还有一大堆神经网络的参数。你还需要一些能运行神经网络的代码，这些代码被包含在我们所说的运行文件中。这个运行文件可以是 C 语言或 Python，或任何其他编程语言编写的。它可以用任何语言编写，但 C 语言是一种非常简单的语言，只是举个例子。只需大约 500 行 C 语言代码，无需任何其他依赖，就能构建起神经网络架构，并且主要依靠一些参数来运行模型。所以只需要这两个文件。
  
  你只需带上这两个文件和你的 MacBook，就拥有了一个完整的工具包。你不需要连接互联网或其他任何设备。你可以拿着这两个文件，编译你的 C 语言代码。你将得到一个可针对参数运行并与语言模型交互的二进制文件。
  
  比如，你可以让它写一首关于 Scale AI 公司的诗，语言模型就会开始生成文本。在这种情况下，它会按照指示为你创作一首关于 Scale AI 的诗。之所以选用 Scale AI 作为例子，你会在整个演讲中看到，是因为我最初在 Scale AI 举办的活动上介绍过这个话题，所以演讲中会多次提到它，以便内容更具体。这就是我们如何运行模型的方式。只需要两个文件和一台 MacBook。
  
  我在这里稍微有点作弊，因为这并不是在运行一个有 700 亿参数的模型，而是在运行一个有 70 亿参数的模型。一个有 700 亿参数的模型运行速度大约会慢 10 倍。但我想给你们展示一下文本生成的过程，让你们了解它是什么样子。所以运行模型并不需要很多东西。这是一个非常小的程序包，但是当我们需要获取那些参数时，计算的复杂性就真正显现出来了。
  
  那么，这些参数从何而来，我们如何获得它们？因为无论 run.c 文件中的内容是什么，神经网络的架构和前向传播都是算法上明确且公开的。<video controls><source src="https://video.twimg.com/ext_tw_video/1728958654978662402/pu/vid/avc1/480x270/QliDORpk5ObHEY2_.mp4?tag=14" type="video/mp4"><source src="https://video.twimg.com/ext_tw_video/1728958654978662402/pu/pl/k8w9wVWIFNn8ZuK3.m3u8?tag=14&container=fmp4" type="application/x-mpegURL"><source src="https://video.twimg.com/ext_tw_video/1728958654978662402/pu/vid/avc1/640x360/JbndtmcyhvYuTiiI.mp4?tag=14" type="video/mp4"><source src="https://video.twimg.com/ext_tw_video/1728958654978662402/pu/vid/avc1/1920x1080/o8p9XDK7AYRqoLQh.mp4?tag=14" type="video/mp4"><source src="https://video.twimg.com/ext_tw_video/1728958654978662402/pu/vid/avc1/1280x720/qV9VD_leMsnjXB-8.mp4?tag=14" type="video/mp4">Your browser does not support the video tag.</video> ([View Tweet](https://twitter.com/dotey/status/1728959646138880026))
- > ## LLM Training
  
  但真正的关键在于这些参数，我们如何得到它们？所以，为了获得模型参数，所谓的模型训练过程比我之前展示的模型推断要复杂得多。模型推断只是在 MacBook 上运行模型。而模型训练则是一个计算上极为复杂的过程。简单来说，我们所做的可以被理解为对大量互联网内容的压缩。
  
  因为 Llama 2 70B 是一个开源模型，我们对其训练方式有相当深入的了解，这得益于 Meta 在论文中公开的信息。以下是一些相关的数据。你需要从互联网上获取大约 10 TB 的文本，通常这些文本来自于对互联网的爬取。想象一下，从各种不同的网站上收集大量的文本，并将它们汇集起来。接下来，你需要获取一大块互联网数据，然后，你需要配置一个 GPU 集群，这些 GPU 是为了处理像神经网络训练这样复杂的计算任务而专门设计的高性能计算机。
  
  你需要大约 6,000 个 GPU，并且需要运行大约 12 天才能得到一个 Llama 2 7B，整个过程大约需要花费 200 万美元。这个过程基本上就是将这大量的文本压缩成你可以想象的一种 zip 文件。我在早些时候的幻灯片中向你展示的这些参数，可以被理解为互联网的 zip 文件。例如，在这种情况下，最终生成的是 140GB 的参数。大致来说，这里的压缩比率达到了大约 100 倍。
  
  但这种压缩与 zip 文件不同，因为 zip 文件是无损压缩，而这里是有损压缩。我们只是大致获取了我们训练文本的概念，而不是在这些参数中保留了文本的完整副本。所以，可以把它理解为一种有损压缩方式。另外需要指出的是，按照目前最先进技术的标准，这些数据其实只是入门级别的。如果考虑到像 ChatGPT、Claude 或 Bard 这样的顶尖神经网络，这些数字可能需要增加十倍甚至更多。
  
  这意味着在实际操作中，我们需要将这些数字大幅上调。这也解释了为什么如今这些神经网络的训练成本高达数千万甚至数亿美元，它们需要庞大的计算集群和大量数据集，而且在获取参数的过程中需要付出巨大努力。一旦获得了这些参数，实际运行神经网络的计算成本就相对较低了。
  
  那么，这个神经网络到底在做什么呢？正如我之前提到的那些参数，神经网络的主要任务其实是预测文本序列中的下一个词。你可以这样理解：当你输入一连串词语，比如 "cat sat on a"，这些词就会被送入神经网络。神经网络中分布着的这些参数，就是完成这一任务的关键。通过神经元的相互连接和激发，来预测下一个单词。
  
  你可以这么理解这个过程：输入一段文本后，神经网络会预测下一个词是什么。举个例子，在 "cat sat on a" 这四个
  
  词的上下文中，神经网络可能会预测下一个词是“mat”，并且给出了 97% 的高概率。这就是神经网络要解决的核心问题。从数学上可以证明，预测与数据压缩之间存在密切联系。这也是为什么我会说，这种神经网络训练在某种意义上是一种数据压缩：因为如果你能够非常准确地预测下一个词，你就可以利用这个能力来压缩数据集。
  
  所以，这其实是一个专注于预测下一个词的神经网络。你输入一些词，它就会告诉你接下来的词是什么。这种训练的结果之所以显得有些神奇，是因为尽管下一个词预测看似是一个简单的任务，但实际上它是一个非常强大的目标。因为这个目标迫使神经网络在其参数中学习到大量关于世界的信息。
  
  我举个例子，我在准备这个演讲时随机找了一个网页。这个页面是从维基百科的主页抓取的，讲的是 Ruth Handler 的故事。所以，想象一下你是神经网络，你需要根据给定的词来预测下一个词。在这个例子中，我用红色标出了一些信息量很大的词。例如，如果你的目标是预测下一个词，那么你的参数必须要学习很多这样的知识。你得知道 Ruth Handler 是谁，她何时出生，何时去世，她是谁，她的成就等等。在这个预测下一个词的任务中，你实际上学到了大量关于世界的知识，所有这些知识都被压缩到权重和参数中。
- > ## How do they work?
  
  好了，让我们换个话题，来看看这个神经网络是怎么运作的？它是如何完成下一个词预测任务的？它内部的运作机制是什么？这里的情况稍微复杂一些。如果我们放大神经网络的简化图，这有点像是神经网络的示意图。这就是我们称之为 Transformer 的神经网络架构，这是它的一个示意图。现在，这个神经网络的一个显著特点是，我们对其架构有着完整的理解。我们清楚地知道在它的各个阶段会发生哪些数学运算。
  
  但问题在于，这 1000 亿个参数分散在整个神经网络中。因此，基本上，这上千亿个参数散布在整个网络中，我们所了解的只是如何逐步调整这些参数，以使整个网络在下一个词预测的任务上表现得更好。我们知道如何优化这些参数，也知道如何随时间调整它们以获得更佳的下一词预测效果，但我们并不真正清楚这些参数具体是如何工作的。我们可以观察到它在下一个词预测方面的进步，但并不清楚这些参数是如何协同工作以实现这一点的。我们手头有些模型，可以让我们从宏观层面思考网络可能在做的事情。
  
  我们大致理解，它们构建并维护了某种知识库，但这个数据库却非常奇特、不完美且怪异。最近有一个广为流传的例子，我们称之为“反转诅咒”。比如，如果你和目前最先进的语言模型 GPT-4（ChatGPT 的一部分）对话，你问，谁是汤姆·克鲁斯的母亲？它会告诉你是玛丽·李·菲弗，这是正确的。但如果你问，谁是玛丽·菲弗的儿子，它会告诉你它不知道。这种知识很古怪，它似乎是单向的。这些信息并不是简单存储后就能从各种角度获取，你必须从某个特定的角度去提问。
  
  这真是既奇怪又令人困惑。归根结底，我们实际上并不真正了解其工作原理，只能大致判断它是否有效，以及有效的可能性有多大。简而言之，可以将大语言模型 (LLM) 视为难以完全解读的产物。它们与你可能在工程学科中建造的任何其他东西都不相似。它们不像汽车，我们了解汽车的每一个部件。
  
  它们是这些来自长期优化过程的神经网络。我们目前并不完全理解它们是如何工作的，尽管有一个叫做可解释性或机械可解释性的领域，正在尝试研究并理解这些神经网络的每一个部分。目前，我们可以在一定程度上做到这一点，但还未能全面实现。现在，我们主要将它们视为基于经验的产品。我们可以给它们输入一些数据，然后测量输出结果。我们基本上可以测量它们的行为表现。我们可以观察它们在许多不同情况下生成的文本。因此，我认为这需要
  
  相应的复杂评估来处理这些模型，因为它们主要是基于经验的。
- > ## Summary so far
  
  构建像 ChatGPT 这样的模型包括两个主要阶段：预训练和微调。预训练阶段需要从互联网上搜集大量文本资料，使用GPU集群进行处理。这些高性能计算机的成本非常昂贵，通常需要几百万美元的投入。完成后，就得到了基础模型。由于这个过程计算量巨大且成本高昂，公司通常一年或几个月才会做一次。微调阶段相对便宜，需要编写标注指南和雇佣人员进行帮助。例如，可以通过Scale AI等公司进行文档标注。这个阶段需要收集约100,000个高质量的问答回应样本，成本要低得多，可能只需一天就能完成。接下来是进行大量的评估工作，部署模型，并监控和收集任何不当行为。对于每个不当行为，都需要修复并返回第一步重复这个过程。修复方法通常是找到错误回应的对话，然后用正确的回应替换。由于微调成本较低，可以每周或每天进行迭代，许多公司在微调阶段而非预训练阶段会更频繁地进行迭代。
  
  Meta发布的Llama 2系列包括基础模型和助手模型。基础模型无法直接使用，因为它们无法直接对问题回复正确的答案，而助手模型则可以直接进行问答。Meta已经完成了极其昂贵的预训练阶段，提供了基础模型，允许用户基于这些结果进行自己的微调。此外，还有一个你可以选择进行的第三阶段微调，即人类反馈强化学习（RLHF），主要通过使用比较标签来提升额外性能。在OpenAI，这个过程被称为人类反馈强化学习（RLHF），这其实是一个可选的第三阶段，它能在大语言模型中提升额外性能，主要是通过使用比较标签。例如，OpenAI的InstructGPT项目就是这样的一个例子。
- > **LLM Scaling Laws**
  
  好的，现在我要转换话题，我们将讨论语言模型，它们是如何改进的，以及这些改进将带我们走向何方。首先要理解的关于大语言模型的重要概念是“规模化法则”。事实证明，这些大语言模型在预测下一个词的准确性方面的表现是一个非常平滑、规律性强，并且只有两个变量的预测函数。一个变量是 N，即网络中的参数数量；另一个变量是 D，即你用来训练的文本量。只要有了这两个数据，我们就能非常准确地预测你在下一词预测任务上能达到的准确度。令人惊奇的是，这些趋势看起来并没有出现停滞或达到极限的迹象。这意味着，如果你在更多文本上训练更大规模的模型，我们可以非常自信地预期下一词预测的表现将会提升。
  
  因此，在算法上取得进步并非必要条件。算法进步当然是很好的加分项，但我们能够在不增加成本的情况下获得更强大的模型，因为我们只需要更强大的计算机。我们有理由相信这是可行的，并且我们可以在更长时间内训练一个更大的模型。我们非常有信心我们将得到更好的结果。当然，在实际操作中，我们并不是真的那么关心预测下一个词的准确度。但是，从经验上看，这种准确度与我们真正关心的许多评估指标是相关的。例如，你可以对这些大语言模型进行很多不同的测试。你会看到，如果你训练一个更大的模型更长的时间，例如，在 GPT 系列中从 3.5 提升到 4，所有这些测试的准确性都会提高。所以，当我们训练更大规模的模型和更多的数据时，我们自然而然地期待有性能的提升。这正是当前计算领域的一场淘金热的根本驱动力，每个人都在努力获取更强大的 GPU 集群，收集更多的数据，因为人们有很大的信心，通过这样做，可以获得更优秀的模型。算法的进步就像是额外的奖励，许多机构对此投入巨大。但从根本上说，扩大规模提供了一条通往成功的确定途径。
  
  **Tool Use (Browser, Calculator, Interpreter, DALL-E)**
  
  接下来，我想通过一些具体的例子来讲解这些语言模型的能力，以及它们是如何随着时间发展的。不是泛泛而谈，我会用一个具体的例子，逐步分析来说明。所以我打开 ChatGPT，给出了以下的查询。我说，收集关于 Scale AI 及其融资轮次的信息，包括发生的时间、日期、金额和估值，并将这些信息整理成一张表。根据我们收集的大量数据，ChatGPT 在微调学习阶段就已经理解，在这种类型的查询中，ChatGPT 不会仅仅依靠自己作为一个大语言模型来直接回答问题。相反，它学会了在需要时使用一些外部工具来帮助完成任务。在这个例子中，一个很合适的工具就是浏览器。
  
  假设你和我遇到同样的问题，你可能会选择上网搜索，对吧？ChatGPT 做的正是这样的事情。它能够发出特定的词汇，我们可以通过这些词汇观察它是如何尝试进行搜索的。在这种情况下，我们可以拿着这个查询去 Bing 搜索，查看搜索结果。就像你我在浏览搜索结果一样，我们可以把搜索到的文本反馈给语言模型，让它基于这些文本生成回答。这个过程非常类似于我们使用浏览器进行研究的方式。然后，它会将这些信息整理成以下形式，并以此方式进行回应。
  
  所以，它收集了信息，我们得到了一张表格，上面列出了 A、B、C、D 和 E 轮融资的具体日期、筹资金额和对应的估值。接着，它还提供了引用链接，你可以通过这些链接去核实这些信息的准确性。在底部，它表示，实际上我要道歉，我没有找到 A 轮和 B 轮的估值数据，只找到了筹集的金额。所以你可以看到表格中有一项标记为不可用。好的，我们现在可以继续这种互动。我提出，让我们尝试基于 C 轮、D 轮和 E 轮中看到的比例，来推测或估算 A 轮和 B 轮的估值。可以看到，在 C、D 和 E 轮中，筹资金额和估值之间存在一定的比例关系。那么，我们该如何解决这个问题呢？当我们尝试推算“不可用”的数据时，并不是仅凭脑海中的计算就能解决。你不可能只是试图在你的脑海中解决它。这样做相当复杂，因为我们在数学方面并不特别擅长。同样，ChatGPT 也不是通过单纯“思考”就能擅长数学运算。实际上，ChatGPT 知道它应该使用计算器来处理这类任务。因此，它会发出特定的指令，告诉程序它需要使用计算器来计算这个数值。它实际上做的是，首先计算所有的比率，然后根据这些比率来推算 A 轮和 B 轮的估值，可能是 7000 万或者 2.83 亿。所以，现在我们的目标是得到所有不同融资轮次的估值数据。接下来，我们将这些数据制作成一个二维图表：横轴表示日期，纵轴显示 Scale AI 的估值。为了更精确地展示，我们会在纵轴上使用对数刻度，并且加上网格线，使图表看起来既专业又美观。ChatGPT 实际上可以使用工具，这次是编写代码，使用 Python 语言中的 Matplotlib 库来绘制这些数据。它会进入一个 Python 解释器，输入所有数据，然后生成图表。这就是图。它清晰地展示了底部的数据，完全按照我们用自然语言提出的要求制作完成。
  
  与 ChatGPT 交流就像与人交谈一样自然。现在我们看着这张图表，想要进行更多的分析。比如，我们现在想在这个图表上加一条线性趋势线，并尝试推算 Scale AI 到 2025 年底的估值。再比如，在图表上标出今天的日期，并基于趋势线来估算今天和 2025 年底的估值。ChatGPT 完成了所有编码工作，虽然这些代码没有展示出来，但它提供了详细的分析结果。在图表的底部，我们可以看到日期和估值的推算结果。根据这个趋势线的拟合结果，今天 Scale AI 的估值大约是 1500 亿美元。而到了 2025 年底，这个公司预计会成长为价值高达 2 万亿美元的科技巨头。所以，祝贺 Scale AI 团队。但这只是 ChatGPT 擅长的分析类型之一。我想通过这个例子展示的核心点是，语言模型在工具使用方面的能力以及它们的发展趋势。它们的功能不再局限于在大脑中处理信息和选择词汇。如今，它们开始利用各种工具和现有的计算基础设施，将一切紧密联系并用词汇交织在一起，如果这有意义的话。
  
  因此，工具使用已成为这些模型日益增强能力的重要一环。它们能够编写大量代码、进行全面分析、从互联网上检索信息等等。再举一个例子。根据上述信息，试图生成一个代表 Scale AI 公司的图像。所以，基于上面的所有内容，依据大语言模型的上下文理解，它对 Scale AI 有深刻的了解。它可能还记得关于 Scale AI 的一些信息以及网络中储存的知识。然后它去使用另一个工具，在这种情况下这个工具是 DALL-E，这也是 OpenAI 开发的一种工具，可以根据自然语言描述生成图像。所以在这里 DALL-E 被用作生成图像的工具。希望这个演示能具体说明问题解决过程中大量使用工具的情况，这与人类解决很多问题的方式高度相关。我们在解决问题时不仅仅依赖思考，而是广泛运用各种工具，比如电脑就非常有用。对于大语言模型也是如此，利用工具正逐渐成为大语言模型发展的一个重要方向。<video controls><source src="https://video.twimg.com/ext_tw_video/1729276760850563072/pu/vid/avc1/480x270/uTumH9wXycW-y6W1.mp4?tag=14" type="video/mp4"><source src="https://video.twimg.com/ext_tw_video/1729276760850563072/pu/vid/avc1/640x360/lXoIl8MdhFnKbskk.mp4?tag=14" type="video/mp4"><source src="https://video.twimg.com/ext_tw_video/1729276760850563072/pu/vid/avc1/1280x720/r774X0tzyrvl10hS.mp4?tag=14" type="video/mp4"><source src="https://video.twimg.com/ext_tw_video/1729276760850563072/pu/vid/avc1/1920x1080/5gcDdX8ZfZVHH7FL.mp4?tag=14" type="video/mp4"><source src="https://video.twimg.com/ext_tw_video/1729276760850563072/pu/pl/19UQ4xFQ7JNIMmbH.m3u8?tag=14&container=fmp4" type="application/x-mpegURL">Your browser does not support the video tag.</video> ([View Tweet](https://twitter.com/dotey/status/1729277775205335321))
- Sweep AI 实践总结
	- Prompting
		- 使用 `tag`作为锚点，加强CoT的能力
		- ```
		  # Instructions
		  Extract code verbatim from the snippets above. These snippets will be used later to refactor the code according to the user request.
		  * Choose specific and very informative names for these functions under new_function_name.
		  * We must copy the code verbatim, so any extra leading or trailing code will cause us to fail.
		  * Keep whitespace and comments.
		  * Use EXTRACT to isolate specific code segments from the current function and place them into new, separate functions.
		   
		  Respond in the following format:
		   
		  <contextual_request_analysis>
		  Analyze the user request and outline the first and last few lines of code that should be extracted.
		  </contextual_request_analysis>
		   
		  <new_function_names>
		  "new_function_name"
		  ...
		  </new_function_names>
		   
		  <extractions>
		  <<<<<<< EXTRACT
		  first few lines to be extracted from original_code
		  ...
		  last few lines to be extracted from original_code
		  >>>>>>>
		  ...
		  </extractions>
		  ```
	- context window
		- 模型会有最大 token 限制，而且context 在到达最大值的情况下，LLM 的能力反而会弱化。所以需要做一些工程上的优化：找到模型能力最佳的 token 值，在 context 到达一个阈值后，开始裁剪 context 的内容。裁剪的阈值可以设置为 context window 一半。
	- lexical search 基于代码的词法搜索
		- 实现了一个 [工具](https://github.com/sweepai/sweep/blob/main/sweepai/core/lexical_search.py#L12-L57)，可以支持计算TF-IDF。结合向量搜索可以更加好。
		- `N-grams`也是一个可以考虑的方向
	- AST图空间增强
		- 为代码库创建一个 AST 语法空间
		- 从 context 中的代码中，通过 CST 获取到 entity，这样就得到了一个 AST 图空间的 root node
		- 在AST 图空间中检索关联的其他 entity
		- 得到一个高召回率、低精度的内容
		- 通过 LLM workflow，提高精度
		- 高召回率、低精度--> LLM workflow--> 高召回、高精度
		- > LLM 擅长做关联性的匹配,所以用 LLM 可以提高精度.
		-
		-
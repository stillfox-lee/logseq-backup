- [[embedding]] 的基本原理和搜索中的应用
	- embedding 是在计算机领域中，为了解决相似性匹配的一种手段。计算机在判定两个事物是否相似的场景中，必须将事物以某种方式先数字化，再找到一个算法来通过比较数字化的事物来判断它们是否相似。这是一个类似人脑判断的思路，但是，计算机能做的只是模拟人脑的判断。
	- embedding 就是一个数字化的方案。它通过对物理世界建模，将物理上的事物结合业务的场景，建立一个meaning space。
		- 可以参考这个例子
			- ((66a707d3-2ae4-43c9-9372-dbd1a36b7dbe))
	- embedding 的过程是一个抽象的过程，同时也可能是信息损失的一个过程，未映射到 meaning space 维度中的信息就被丢失了。
	- embedding 在文本相似性搜索中的工作流程
		- 文本向量化。通过`TF-IDF`算法计算向量，或者通过GPT、BERT 这种模型计算向量值。
		- 查询向量化。提取查询中的关键字，对关键字计算向量。
		- 查找在向量空间中，查询向量与文本向量坐标最相近的向量值。
			- 衡量两个向量是否相近的方法
				- 余弦相似度：计算向量之间的夹角余弦值。也就是向量点积。
				- 欧式距离：计算两个向量空间中的直线距离。
				- 曼哈顿距离：计算两个向量在各个维度上差值的绝对值之和。
				- > 应该没有最优的解决方案，工程实现中存在很多优化的空间。
			- 相似度查找的工程优化：
				- 找到相似度高的文本，看起来需要遍历所有的文本向量。然后再逐个与查询向量计算点积值。工程上不太可行。
				- 可以为文本向量建立索引，然后通过索引快速查询相识值。
				- 常规向量索引：
					- KD 树
					- 局部敏感哈希（LSH）
					- 近似最近邻（ANN）
	- embedding 的过程可以是通过 embedding 模型来完成的。以文本的 embedding 为例，两个文本或者词在模型语料库中表现为同义的话，它们在 meaning space 中的坐标会非常接近。所以搜索中的相似性匹配和 embedding 模型训练数据集也有关系，如果模型不能理解业务意义，那么这个业务领域中的同义词在 meaning space 中的坐标距离可能会比较远了。（如果没有水果相关的语料库，那么樱桃和车厘子在 meaning space 中可能比较远）
	-
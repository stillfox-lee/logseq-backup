- RLHF (Reinforcement Learning from Human Feedback)
	- 为什么需要 RLHF？
	- 因为通过 Pre-train 我们只能得到一个BaseModel，它不一定能够符合我们 AI 助手对话的使用场景。需要通过 fine-tune 才可以做到从 BaseModel 到 ChatModel。
	- [OpenAI 发布的 RLHF 论文](https://arxiv.org/abs/2203.02155)
	- [anthropic 关于对齐的论文](https://arxiv.org/pdf/2112.00861)
	- RLHF 主要需要用到的模型：
		- BaseModel
		- Supervised fine-tuning (SFT) model (从 BaseModel 得到)
		- Reward model (from SFT)
	- SFT Model
		- 这个模型和预训练基座模型类似，只是数据量比较小。微调数据主要是**期望**的对话数据集。
		- 不过 SFT 的质量也不一定是很好的，模型在这里还很有可能会“说谎”。
	- Reward Model
		- 为了解决 SFT 过程中的问题，需要利用`强化学习`，也就是 RL。这个模型主要的工作就是模拟人类对 SFT 模型输出的结果进行*打分*。
	- RLHF
		- 进一步微调 SFT model，将 Reward Model 判断得到的知识融入到 SFT Model中。
		- 从大量的任务中随机选择一个任务，将它的 prompt 提交给 SFT Model，让它生成一个结果。然后由 Reward Model 完成这个打分。从而调整 SFT model 的权重。
-
- LLM [RAG] #LLM
	- refer
		- {{video https://www.youtube.com/watch?v=bjb_EMsTDKI&list=PLfaIDFEXuae2LXbO1_PKyVJiQ23ZztA0x&index=2}}
		- https://github.com/langchain-ai/rag-from-scratch/blob/main/README.md
		- slide：https://docs.google.com/presentation/d/1MhsCqZs7wTX6P19TFnA9qRSlxH3u-1-0gWkhBiDG9lQ/edit
		-
	- ![](https://raw.githubusercontent.com/stillfox-lee/image/main/picgo/202403010945973.png){:height 346, :width 748}
	- Index
	- Retrieval
	- Generation
	- Question Translation
		- 这是 RAG Pipeline 的第一步，通过将 User 输入的 Question 进行 Translate，以提高在 Retrieval 中的效果。
		- Translate Questions 的方法：
		  ![](https://raw.githubusercontent.com/stillfox-lee/image/main/picgo/202403050910101.png)
		-
		- Multi-Query
			- > 主要思路是：增加多个角度来提高命中的数量，最后将结果整合，作为答案。
			- 通过多个角度提升准确性
			  ![](https://raw.githubusercontent.com/stillfox-lee/image/main/picgo/202403060909300.png){:height 617, :width 688}
			- 整体的过程：
			  ![](https://raw.githubusercontent.com/stillfox-lee/image/main/picgo/202403050942480.png)
			- 将一个 Question，通过 LLM 重写为多个 Q。然后通过相关性搜索出多个 Q 关联的多个doc。再将多个 doc 和原始 Question发给 LLM，让 LLM 来完成 Reasoning。
		- RAG Fusion
			- ref：
			  https://github.com/Raudaschl/rag-fusion
			  https://towardsdatascience.com/forget-rag-the-future-is-rag-fusion-1147298d8ad1
			  https://arxiv.org/pdf/2402.03367.pdf
		- Decompostion
			- 前面的方法都是做Re-written。为用户输入的问题提供更多的角度以提供准确率。**Decomposition**的思路是：将一个问题分解为多个小的子问题，解决每个子问题后，再整合解决这个原始问题。
			- 涉及的技术有：`Least-to-Most` `IR-CoT`
			- ref:
			  [IR-CoT paper](https://arxiv.org/pdf/2212.10509.pdf)
			  [Least-To-Most paper](https://arxiv.org/pdf/2205.10625.pdf)
			- 实现方案有两种：
			- 1. 子问题拆分为前后相关，逐步解答每个子问题。同时将前一个子问题的 context 带入到下一个子问题中一起推理。
			  ![](https://raw.githubusercontent.com/stillfox-lee/image/main/picgo/202403070949169.png)
			- 2. 子问题是独立的，独立解答子问题后，汇总给 LLM 去推理。
			  ![](https://raw.githubusercontent.com/stillfox-lee/image/main/picgo/202403070949320.png)
			-
			-
		- Step-back prompting
			- ref
				- [step back: evoking reasoning via abstraction in llm](https://arxiv.org/pdf/2310.06117.pdf)
			- 这个方法的整体思路是：遇到一个复杂的问题时，不是直接尝试回答这个问题，而是先退后一步。提出一个或者多个与原问题相关但更基础或者更容易回答的问题。通过回答这些 step-back 问题，系统可以获得更多相关知识，从而更好地理解原始问题。
			- 实现
				- 通过`few-shot`prompting，让 LLM 给出原始问题对应的 step-back 问题。
				- step-back questions 和文档做retrieval
				- 整合retrieval后的信息、原始信息，给 LLM 做reasoning。
		- HyDE(Hypothetical Document Embeddings)
			- > 特别是在处理那些没有直接可用文档或数据源来回答用户查询的情况下。这种技术通过生成一组假设性的文档或文本片段，来模拟可能包含查询答案的文档内容。然后，这些假设性文档被嵌入到向量空间中，以便进行进一步的处理和分析。
			- ref:
				- [Precise Zero-Shot Dense Retrieval without Relevance Labels](https://arxiv.org/pdf/2212.10496.pdf)
			- ![](https://raw.githubusercontent.com/stillfox-lee/image/main/picgo/202403080956427.png)
			-
			-
- kube-scheduler
	- pod 有 priority，高 priority 的 pod 会得到优先调度。
	- 调度 pod 的流程：scheduler *track nodes* --> predicate  --> score --> schedule pod
	- 运维人员如何控制：
		- 通过`label`、`podAffinity`、`taints`、`Tolerant`
		- 可以通过`Policy` 来定制 **predicate**和**priorities**等影响 scheduler 行为
	- scheduler 在整个流程中，有 **12**个可拓展的节点，k8s 支持在其中使用插件来定制调度器。
	- 集群 node 数量大的情况下，调度器需要 track 的内容变多，会有性能损失。
- 服务发现与平滑发布的处理 #service-descovery
	- 上线
		- 先向注册中心 register
		- consumer 通过 health check 检测正常，再加入到 LB 中
	- 下线
		- 收到 signal
		- 向注册中心 deregister
		- 关闭 health check 的响应
		- 等待 2-3 轮心跳检测周期后再退出 *(等 consumer 把节点从 LB 中去掉)*
- 自己实现[[service-descovery]]
	- 注册中心
		- 实际需要是一个分布式的数据库。可以是一个 AP 的系统，牺牲掉一部分的一致性，在这个场景中是可以接受的。**这个注册中心本身的分布式问题**
		- 注册中心与 Provider 之间有 heartbeat 检测，故障的 Provider 会踢下线。
			- #+BEGIN_CAUTION
			  如果出现Provider 于注册中心出现了网络分区，但是 Consumer 与注册中心是正常的。由于 Consumer 需要定期与 Provider 同步数据，这样会使得部分的 Provider 被不正常的下线了。
			  #+END_CAUTION
			- #+BEGIN_CAUTION
			  或者Provider 与注册中心的网络抖动，心跳不稳定。导致 Provider 频繁上、下线。
			  #+END_CAUTION
			- 这些都是属于网络分区的问题，我认为可以通过在注册中心层面通过多节点*跨网络*来解决。比方说，Register 时收集网络信息，如果某个data-center的 Provider **短时间**、**大批量**不健康，那么可以在注册中心的多个节点间进行double-check。*实现了多网络的心跳检测*。
			-
	- 功能
		- Provider
			- **register**
				- register 的 metadata 可以按照业务写入其他信息：`集群`、`优先级`、`流量染色标签`等。
			- **health check**
			- **deregister**
		- Consumer
			- pull
				- 获取信息应该是需要周期性的向注册中心获取，并且及时更新负载均衡池的。这里面有很多的技术细节。
		- Load balance
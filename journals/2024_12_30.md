- 3blue1brown [[Transformer]] 注意力机制讲解 #AI #LLM {{video https://www.bilibili.com/video/BV1TZ421j7Ke/?spm_id_from=333.999.0.0&vd_source=2abff075bda6eb4c91c27fd1da6f2b36}}
	- ## 高屋建瓴理解一下 Attention
		- 通过 Embedding 模块，对输入的文本内容进行“上下文”关联的计算。这个计算是通过在 Embedding 的高维空间里面不断根据上下文去更新每个“token”的 vectors 值，为它赋予具有上下文意义的 vector 值。
		- 一个例子就是：`Transformer 是一个很好的模型`。这句话中`Transformer`如果在“动画”、“影视”的上下文中，它的 vector 值应该被更新为`变形金刚`；如果是在”大模型“上下文中，它应该被更新为 `Transformer Model`。
	- 如何理解 context window
		- ![](https://raw.githubusercontent.com/stillfox-lee/image/main/picgo/202412301433291.png)
		- 假设我们输入了一整本的推理小说内容，然后最后问模型一个问题：**凶手是__**。
		- 那么就需要通过 Attention 模块，将所有的 context window 中信息计算一遍，得出下一个词的概率。
	- 为此，Transformer 设计了如下的几个部分完成。
	  ![](https://raw.githubusercontent.com/stillfox-lee/image/main/picgo/202412301427514.png)
	-
	-
	- ## 第一部分 Embedding 模块
		- 将所有的文本内容，转换为向量。
	- ![](https://raw.githubusercontent.com/stillfox-lee/image/main/picgo/202412301414967.png)
	-
	- ## 第二部分 Attention 计算
		- 为每个 token 添加上下文”意义“。将 vectors 值，通过结合上下文，得出更加具有”意义“的token。为最后的预测做准备。
		- 具体步骤：
			- 通过 **Attention Pattern**，将 `Query`矩阵与`Key`矩阵进行`点积`计算后、softmax 处理，得到了每个 token 对应的当前上下文token 的相关性。这个 Attention Pattern 计算得到的，就是Query 和 Key的`注意力权重`。
			  ![](https://raw.githubusercontent.com/stillfox-lee/image/main/picgo/202412301619078.png)
			-
			- 然后就是更新 token 对应的 vector 值。通过 Embedding 获得上一步中更多的上下文的vector 值更新对应的 token。这就需要一个可以加的向量值，通过 `旧 vector+偏移=新 vector`。
			- 这个偏移的值，通过`注意力权重`再点积`V`矩阵得到。
			- 上面的步骤，其实就是论文公式：
			  ![](https://raw.githubusercontent.com/stillfox-lee/image/main/picgo/202412301659965.png)
			-
		- 整个过程可以举例理解：”我喜欢 Transformer“。如果全文是”在神经网络中，我喜欢 Transformer“，那么这个 Transformer 就对应了“神经网络”的意义。如果全文是“在动画电影中，我喜欢 Transformer”。那么这个 Transformer 就对应了“变形金刚”的意义。
		- 而这 3 个矩阵 K、V、Q 也是通过模型训练得到的可调参数的矩阵算出来的。
		  ![](https://raw.githubusercontent.com/stillfox-lee/image/main/picgo/202412301704415.png)
		-
		-
	- ## 第三部分 MLP 多层感知机
	-
- [[chatgpt-prompt-engineering]] #prompt
	- Base LLM VS Instruction Tuned LLM [[LLM]]
		- 后者通过 [[RLHF]] (Reinforcement Learning with Human Feedback) 做了 Fine-tune。能够更好地接受 instruction。
		- ![](https://raw.githubusercontent.com/stillfox-lee/image/main/picgo/202311070827659.png)
		-
	- Guidelines for [[prompt]]
		- ### 1. write clear and specific instructions
			- `clear != short` 长的 prompt 可以给模型更多的context，让模型更好地给出内容。
			- **Tactic 1**
				- 使用分隔符，标注不同的信息，让大模型能够便于理解。
				  ```
				  """
				  ---
				  <>
				  <tag> </tag>
				  ```
				- 例如在翻译或者总结的场景下。可以使用分隔符**明确**告诉模型，哪些是需要被翻译和总结的内容，哪些是 prompt 的说明内容。
			- **Tactic 2** 让大模型做一个结构化格式化的输出。例如格式化 JSON 等。
				- ![](https://raw.githubusercontent.com/stillfox-lee/image/main/picgo/202311080927372.png)
			- **Tactic 3** 让大模型做特定的条件检查
				- 这个例子里面：首先使用了 tactic1，格式化标识了需要被处理的内容，然后要求大模型检查内容，是否可以提炼出具体步骤。
				- ![](https://raw.githubusercontent.com/stillfox-lee/image/main/picgo/202311080929143.png)
				- 下面是一个没有步骤的内容：
				- ![](https://raw.githubusercontent.com/stillfox-lee/image/main/picgo/202311080930985.png)
				- > 通过这个方式，就可以让大模型处理一些*检测*类的任务。
				-
			- **Tactic 4** Few-shot prompting
				- Give successful examples of completing tasks.Then ask model to perform the task.
				- 在让模型处理之前，先告诉它一个正确的示例。让它能够按照这个示例参考。
				- ![](https://raw.githubusercontent.com/stillfox-lee/image/main/picgo/202311090911825.png)
				- > 上面的例子，给了模型一个示例，同时带有角色和语气。模型给出的结果也是“以奶奶的角度，教授孩子”。同时说话的形式也是和示例类似的。
				-
		- ### 2. Give the model time to think
			- **Tactic 1**
				- 将一个 task 分为几个制定的 step 来完成。在一次 prompt 中，可以指定需要分几个步骤完成这个复杂的任务。
				- ![](https://raw.githubusercontent.com/stillfox-lee/image/main/picgo/202311090910415.png)
				- > 给出了 1、2、3、4 具体的步骤，以及各个步骤要做的事情。
				- ![](https://raw.githubusercontent.com/stillfox-lee/image/main/picgo/202311090914942.png)
				- 结合了`分隔符`、`格式化输出`、`任务拆解`几个技巧。
				-
			- **Tactic 2**
				- Instruct the model to work out its own solution before rushing to a conclusion.
				- **多步指导模型如何得出正确答案，而不是期望它能够一下能够给出正确的答案**
				- 场景：验证学生的数学题解是否正确。prompt 直接给出题目和题解，model 往往不能正确识别出来。可以通过 prompt 来 instruct 模型，让它先分一个 step 给出自己的答案，再来验证学生的题解。
				-
				- ![](https://raw.githubusercontent.com/stillfox-lee/image/main/picgo/202311090939585.png)
				- > 这里可以看出，模型无法直接判断结果
				- ![](https://raw.githubusercontent.com/stillfox-lee/image/main/picgo/202311090952706.png)
	- 模型本身的局限性
		- **Hallucination**
			- 它会编造一些看起是正确的答案，但实际上是错误的。
			- 如何减少这个问题：
			- 让模型先相关的信息，再让它根据这些信息给出答案。
	- Iterative 好的 prompt 是需要迭代的
		- ![](https://raw.githubusercontent.com/stillfox-lee/image/main/picgo/202305120928099.png){:height 710, :width 748}
	- ## summarize text
		- 技巧
			- 指定长度
			- 限定特殊的主题，或者限定给特定角色的人进行总结。
		- 例如，商品的评论信息。可以提炼给物流、运营、市场等各个角度的总结信息。
		  ![](https://raw.githubusercontent.com/stillfox-lee/image/main/picgo/202311100934120.png)
		-
	- ## Inferring
		- 对文字内容做出情感的推断。比如评价是正面还是负面的。
			- ![](https://raw.githubusercontent.com/stillfox-lee/image/main/picgo/202311160924975.png)
			- > 在传统的机器学习中，需要用数据来训练模型，让模型能够识别什么是**正面**、**负面**的词语，再结合 NLP 进行处理，这个场景下就可以看出大模型的厉害之处了。
		- 对文字内容做出 topic 的分析处理。
			- 使用场景
				- 提炼出 topic
				- 给出文字内容，让模型将它分类到既定 topic中
			-
	- ## Transforming
		- 翻译、修正语法错误
		- 将一段文本，为不同的身份场景，针对性转译。
	- ## Expanding
		- 利用temperature，在稳定性和创造性中找到合适的值。
		- ![](https://raw.githubusercontent.com/stillfox-lee/image/main/picgo/202312080911536.png)
		-
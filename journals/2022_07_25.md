- [[Flannel]] UDP 模式 #k8s [[k8s network]]
	- ![flannel udp](https://raw.githubusercontent.com/stillfox-lee/image/main/picgo/20220725091855.png)
	- 通过一种`TUN`隧道，在容器 bridge 与 flanneld 进程之间建立了一个网络隧道。使得 bridge 的数据可以发送到 flanneld 中进行转发处理。
	- 数据包进入到 flanneld 进程之后，会将 3 层的`packet`封装为 4 层的 UDP `datagrams`。再通过 eth0 发送到目标主机。
	- 这种技术的明显弱点是：数据多次的在 user space 与 kernel space 之间传输。
	-
- [[Flannel]] 的 [[VXLAN]]模式 #[[k8s network]]
	- 是一种针对于 UDP 方案的优化。UDP 方案中，UDP 包的封装和解包是在用户态进行的。而 VXLAN 方案是由内核完成这个步骤的。
	- 设计思想：在原本二层中的`frame`封装到四层UDP的`datagrams`。 在现有的三层网络上，“覆盖”一层虚拟的、由内核 VXLAN 模块负责维护的二层网络。属于一种 [[Overlay Network]]技术。
	- VTEP 设备
		- VXLAN tunnel end point
		- 对三层的 IP `packet`进行二层`frame`的封包和解包，为`packet`添加`MAC`地址，将 IP 包转换为一个二层数据帧，以实现 [[Overlay Network]]。
		- 具有 IP 地址和 MAC 地址
		- 由内核模块负责处理网络数据
	- 数据包的流转过程
		- ![](https://raw.githubusercontent.com/stillfox-lee/image/main/picgo/20220729101805.png)
	- DONE 对 flannel 的 VXLAN 总结
	  :LOGBOOK:
	  CLOCK: [2022-07-29 Fri 10:18:15]--[2022-07-29 Fri 10:18:16] =>  00:00:01
	  CLOCK: [2022-07-29 Fri 10:18:22]--[2022-07-29 Fri 10:18:23] =>  00:00:01
	  :END:
		- 为什么需要这个东西？
		- 整个过程中，如何拆包、封包
	-
- [[k8s network]] 的 [[CNI]] 原理
	- CNI 插件的工作：
		- 通过某种方法，将不同的 host 上的特殊设备连通，从而达到容器跨主机通信的目的。
		- 为每个 Pod 保证三层网络可达
	- [[CNI]]插件为 Pod 构建网络流程：
		- 以 CNI bridge 模式为例，介绍工作流程。
		- Pod schedule
		- kubelet create Pause container
			- k8s 会为创建 Pod 的网络栈。
			- k8s 会先创建一个`pause`容器，`pause`容器会创建一个`Network namespace`的网络栈。
				- https://miro.medium.com/max/1400/1*nCuVGFpyic4yaz7EOGKqXQ.jpeg
			- Namespace 网络栈包括
				- 网卡
				- loop back
				- 路由表
				- iptables 规则
			- Pod 中的其他容器，都会加入这个namespace 中。所以，Pod 中的容器可以通过本地通信。而 Pod 之间的通信，就需要靠 CNI 来处理了。
		- kubelet --> CRI --> CNI `Add` command
			- CNI --> Main plugin *(bridge)*
				- **create bridge**
					- 检查**宿主机**上是否存在 bridge，没有则创建它。相当于执行：
					  ```bash
					  ip link add cni0 type bridge
					  ip link set cni0 up
					  ```
				- **create [[veth]] **
					- 进入 `pause`容器的`Network Namespace`，创建一对`Veth Pair`。
					- 用 veth 将容器网络与宿主机网络连接：
					  ```bash
					  #在容器里
					  # 创建一对Veth Pair设备。其中一个叫作eth0，另一个叫作vethb4963f3
					  $ ip link add eth0 type veth peer name vethb4963f3
					  # 启动eth0设备
					  $ ip link set eth0 up
					  # 将Veth Pair设备的另一端(也就是vethb4963f3设备)放到宿主机(也就是
					  Host Namespace)里
					  $ ip link set vethb4963f3 netns $HOST_NS
					  # 通过Host Namespace，启动宿主机上的vethb4963f3设备
					  $ ip netns exec $HOST_NS ip link set vethb4963f3 up
					  
					  # 在宿主机上
					  $ ip link set vethb4963f3 master cni0
					  ```
					- 然后，还需要允许 bridge 开启`Hairpin Mode`。这样才能够允许容器自己访问自己。因为默认 bridge 是不允许数据包从同一个端口同时进出的。
			- Main plugin *(bridge)* --> IPAM plugin
				- bridge 插件再调用 IPAM 插件，通过它完成对容器的名为`eth0`的 veth 设置 IP 和 route。
				  ```bash
				  # 在容器里
				  $ ip addr add 10.244.0.2/24 dev eth0
				  $ ip route add default via 10.244.0.1 dev eth0
				  ```
			- Main plugin *(bridge)* 为宿主机CNI bridge 分配 IP
				- ```
				  # 在宿主机上
				  $ ip addr add 10.244.0.1/24 dev cni0
				  ```
- [[CNI]] 插件类型
	- Main 插件。
		- 用于创建具体的网络设备，如 Bridge、ipvlan、loopback、macvlan、veth pair 等
		- 在宿主机上的`/opt/cni/bin/`就可以看到对应的二进制文件
	- IPAM (IP Address Management)插件。
		- 负责分配 IP 地址，如 dhcp、host-local。
	- 社区 CNI 插件。
		- flannel、tuning、portmap、bandwith 等。
- [[CNI]] 的构成：
	- 一个 CNI 程序二进制文件
		- 给 Pod 配置网络接口，将 Pod 的 pause 容器加入 CNI bridge。
	- 一个管理daemon
		- 管理集群之间的网络互连、路由。
		- 通常用 DaemonSet 部署